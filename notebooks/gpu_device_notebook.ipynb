{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05880649",
   "metadata": {},
   "source": [
    "# GPU Device Detection and Sentiment Analysis\n",
    "\n",
    "Notebook ini mendemonstrasikan cara mendeteksi perangkat GPU yang tersedia (CUDA/MPS/CPU) dan menggunakan pipeline sentiment analysis dari Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf3b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:', torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',\n",
    "          torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional GPU details using GPUtil\n",
    "try:\n",
    "    import GPUtil\n",
    "\n",
    "    gpus = GPUtil.getGPUs()\n",
    "\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU ID:\", gpu.id)\n",
    "        print(\"GPU Name:\", gpu.name)\n",
    "        print(\"GPU Utilization:\", gpu.load * 100, \"%\")\n",
    "        print(\"GPU Memory Utilization:\", gpu.memoryUtil * 100, \"%\")\n",
    "        print(\"GPU Temperature:\", gpu.temperature, \"C\")\n",
    "        print(\"GPU Total Memory:\", gpu.memoryTotal, \"MB\")\n",
    "except ImportError:\n",
    "    print(\"GPUtil not installed. Install with: pip install gputil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1083a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available(): # Check for CUDA sama kaya cuda:0\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available(): # Check for MPS sama kaya mps:0\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"✅ Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e261067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeihanaulia/Programming/research/agent/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff197e2",
   "metadata": {},
   "source": [
    "## Penjelasan Device Setting\n",
    "\n",
    "Apa bedanya kalau di sini device di set dan tidak?\n",
    "\n",
    "- Jika device di set, maka model akan dijalankan di perangkat yang ditentukan (CPU, CUDA, atau MPS).\n",
    "- Jika tidak di set, maka model akan menggunakan perangkat default (biasanya CPU).\n",
    "- Hugging Face (via transformers.pipeline) mendeteksi otomatis bahwa device mps:0 tersedia dan memilihnya sendiri.\n",
    "- Tapi ini bergantung pada versi PyTorch dan Transformers — di beberapa versi lama, kalau lo gak set device, dia default ke CPU, bukan MPS.\n",
    "\n",
    "Maksud dari :0 itu apa ya?\n",
    "- :0 menunjukkan bahwa kita menggunakan GPU pertama (indeks dimulai dari 0). \n",
    "- Jika ada beberapa GPU, kita bisa memilih GPU mana yang ingin digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a9f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998641014099121}]\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I love Hugging Face!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117000bb",
   "metadata": {},
   "source": [
    "## MPS vs CUDA\n",
    "\n",
    "MPS (Metal Performance Shaders) adalah framework akselerasi hardware yang dikembangkan oleh Apple untuk perangkat macOS dan iOS yang menggunakan GPU berbasis arsitektur Metal.\n",
    "\n",
    "CUDA (Compute Unified Device Architecture) adalah platform komputasi paralel dan model pemrograman yang dikembangkan oleh NVIDIA untuk GPU mereka.\n",
    "\n",
    "Keduanya memungkinkan pemrosesan paralel yang efisien, tetapi MPS dirancang khusus untuk perangkat Apple, sedangkan CUDA dirancang untuk GPU NVIDIA.\n",
    "\n",
    "Secara performa, CUDA umumnya lebih matang dan memiliki dukungan yang lebih luas dalam komunitas pembelajaran mesin dibandingkan MPS, karena telah ada lebih lama dan digunakan secara luas di berbagai aplikasi.\n",
    "\n",
    "Perbedaan utama antara MPS dan CUDA terletak pada kompatibilitas perangkat keras, ekosistem perangkat lunak, dan dukungan komunitas.\n",
    "\n",
    "### Referensi\n",
    "- https://www.reddit.com/r/LocalLLaMA/comments/1crwkia/why_people_buying_macs_instead_of_cuda_machines/?rdt=46933\n",
    "- https://pytorch.org/docs/stable/notes/mps.html\n",
    "\n",
    "### Key Quotes from Reddit Discussion:\n",
    "- \"Unified memory allows them to run bigger models in a portable way...\" (LumbarJam)\n",
    "- \"To get 192gb of nvidia vram it's gonna be wildly expensive...\" (Enough-Meringue4745)\n",
    "- \"Unified Memory is the draw here...\" (waitmarks)\n",
    "- \"On Apple silicon maps, the GPU is capable of accessing way more RAM...\" (ervwalter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
